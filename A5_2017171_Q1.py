# -*- coding: utf-8 -*-
"""DT Classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uJ2eT91I-vj1Rjd0nzv5L2HOz16kl59e
"""

import pandas as pd
import numpy as np
import numpy as np
import random
import math

"""**Pre-Processing the data*"""
def take_input(input_path):
	dataset = pd.read_csv(input_path)
	regdata=pd.read_csv(input_path)
	return dataset,regdata
def processing_data(dataset,regdata):	
	dataset['label']=dataset['month']
	df=dataset
	df=df.drop(labels=['month','No'],axis=1)
	median=df['pm2.5'].median()
	df['pm2.5'].fillna(median,inplace=True)
	regdata['label']=regdata['pm2.5']
	df1=regdata
	df1=df1.drop(labels=['pm2.5','No'],axis=1)
	median1=df1['label'].median()
	df1['label'].fillna(median1,inplace=True)
	return df,df1
"""**Train-Test Split**"""

def split_data_haha(df):
  train_df=df.loc[df['year'].isin([2010,2012,2014])]
  test_df=df.loc[df['year'].isin([2011,2013])]
  return train_df,test_df
#Change this function
def types_of_features(df):
  feature_types = []
  for feature in df.columns:
      if feature != "label":
          unique_values = df[feature].unique()
          if (isinstance(unique_values[0], str)):
              feature_types.append("categorical")
          else:
              feature_types.append("continuous")
  
  return feature_types
 
def all_splits(data,rf=False,random_feat=0):
  all_splits = {}
  column_indices = list(range(data.shape[1] - 1))    
  if(rf):
    column_indices = random.sample(population=column_indices, k=random_feat)
  for column_index in column_indices:        
    values = data[:, column_index]
    unique_values = np.unique(values)
    all_splits[column_index] = unique_values
  return all_splits

def split_data(data, split_column, split_value):
    split_column_values = data[:, split_column]
    df=pd.DataFrame(data=data)
    FEATURE_TYPES=types_of_features(df)
    type_of_feature = FEATURE_TYPES[split_column]
    if type_of_feature == "continuous":
        data_below = data[split_column_values <= split_value]
        data_above = data[split_column_values >  split_value]  
    else:
        data_below = data[split_column_values == split_value]
        data_above = data[split_column_values != split_value]
    
    return data_below, data_above

def mse(data):
    actual_values = data[:, -1]
    if len(actual_values) == 0: 
        mse = 0
    else:
        prediction = np.mean(actual_values)
        mse = np.mean((actual_values - prediction) **2)
    return mse

def winner(data, potential_splits,task):
	i=True
	overall_entropy=9999999999999999
	for column_index in potential_splits:
		for value in potential_splits[column_index]:

			data_below, data_above = split_data(data, split_column=column_index, split_value=value)
			label_column_below = data_below[:, -1]
			label_column_above = data_above[:,-1]
			xxx, counts_below = np.unique(label_column_below, return_counts=True)
			yyy, counts_above = np.unique(label_column_above, return_counts=True)
			probabilities_below = counts_below / counts_below.sum()
			probabilities_above = counts_above / counts_above.sum()
			entropy_below = sum(probabilities_below * -np.log2(probabilities_below))
			entropy_above = sum(probabilities_above * -np.log2(probabilities_above))
			total_length = len(data_below) + len(data_above)
			prob_data_below = len(data_below) / total_length
			prob_data_above = len(data_above) / total_length
			if(task=="Classification"):
				current_overall_entropy =  (prob_data_below * entropy_below + prob_data_above *entropy_above)
				if current_overall_entropy <= overall_entropy:
					i=False
					overall_entropy = current_overall_entropy
					best_split_column = column_index
					best_split_value = value
			else:

				current_overall_entropy =  (prob_data_below * mse(data_below) + prob_data_above *mse(data_above))
				if (current_overall_entropy <= overall_entropy):

					i=False
					overall_entropy = current_overall_entropy
					best_split_column = column_index
					best_split_value = value

	return best_split_column, best_split_value

def buildtree(train_df,max_depth,task,rf=False,cur_depth=0,random_feat=0):
    if cur_depth == 0:
        global column_names, FEATURE_TYPES
        column_names = train_df.columns
        FEATURE_TYPES = types_of_features(train_df)
        data = train_df.values
    else:
        data = train_df      

    if (len(np.unique(data[:,-1])) == 1) or (cur_depth== max_depth):
      if(task=="Classification"):
        labelc = data[:, -1]
        unique_classes, counts_unique_classes = np.unique(labelc, return_counts=True)
        index = counts_unique_classes.argmax()
        classification = unique_classes[index]
        return classification
      else:
        classification = np.mean(data[:,-1])
        return classification     
    else:    
        cur_depth =cur_depth+1
        potential_splits = all_splits(data,rf=rf,random_feat=random_feat)
        split_column, split_value = winner(data, potential_splits,task=task)
        data_below, data_above = split_data(data, split_column, split_value)
        feature_name = column_names[split_column]
        type_of_feature = FEATURE_TYPES[split_column]
        if type_of_feature == "continuous":
            question = "{} <= {}".format(feature_name, split_value)
        else:
            question = "{} = {}".format(feature_name, split_value)
        sub_tree = {question: []}
        yes_answer = buildtree(train_df=data_below, max_depth=max_depth,task=task,cur_depth=cur_depth,rf=rf,random_feat=random_feat)
        no_answer = buildtree(train_df=data_above, max_depth=max_depth,task=task,cur_depth=cur_depth,rf=rf,random_feat=random_feat)
        if yes_answer == no_answer:
            sub_tree = yes_answer
        else:
            sub_tree[question].append(yes_answer)
            sub_tree[question].append(no_answer)
        
        return sub_tree

def classify_example(example, tree):
    question = list(tree.keys())[0]
    feature_name, comparison_operator, value = question.split(" ")
    if comparison_operator == "<=": 
        if example[feature_name] <= float(value):
            answer = tree[question][0]
        else:
            answer = tree[question][1]
    else:
        if str(example[feature_name]) == value:
            answer = tree[question][0]
        else:
            answer = tree[question][1]
    if not isinstance(answer, dict):
        return answer
    else:
        residual_tree = answer
        return classify_example(example, residual_tree)
def calculate_metric(train_df,task,test_df,max_depth):

	tree=buildtree(train_df,task=task,max_depth=max_depth)
	copy1=test_df.copy()
	copy1["classification"] = test_df.apply(classify_example, axis=1, args=(tree,))
	if(task=="Classification"):
		copy1["classification_correct"] = copy1["classification"]==copy1["label"]
		accuracy = copy1["classification_correct"].mean()
		return accuracy  
	else:
		copy1["error"]=copy1["classification"]-copy1["label"]
		copy1["sqerror"]=(copy1["error"])**2
		sum1=copy1["sqerror"].sum()
		div=sum1/(test_df.shape[0]-1)
		std_dev=math.sqrt(div)
		emse=copy1["sqerror"].mean()
		print("Mean error is :")
		print(copy1["error"].mean())
		print("Standard Deviation is: ")
		print(std_dev)
		return emse
def bootstrapping(train_df, n_bootstrap):
    bootstrap_indices = np.random.randint(low=0, high=len(train_df), size=n_bootstrap)
    df_bootstrapped = train_df.iloc[bootstrap_indices]
    return df_bootstrapped
def bagging_predictions(df,n_trees,n_bootstrap, max_depth,task,test_df,want_random_forest,random_feat=0): 
    prediction=test_df.copy()
    for i in range(n_trees):
      df_boot=bootstrapping(df,n_bootstrap=n_bootstrap)
      cur_tree=buildtree(df_boot,max_depth=max_depth,task=task,rf=want_random_forest,random_feat=random_feat,cur_depth=0)
      cur_prediction=test_df.apply(classify_example, axis=1, args=(cur_tree,))
      prediction["tree_{}".format(i+1)]=cur_prediction
    if(task=="Classification"):
      prediction["overall_prediction"]=prediction[prediction.columns[-n_trees:]].mode(axis=1)[0]
      prediction["classification_correct"] = prediction["overall_prediction"] == prediction["label"]
      accuracy = prediction["classification_correct"].mean()
      return accuracy
    else:
      prediction["overall_prediction"]=prediction[prediction.columns[-n_trees:]].mean(axis=1)
      prediction["Sq_error"]=(prediction["overall_prediction"]-prediction["label"])**2
      mse=prediction["Sq_error"].mean()
      meanerror=(prediction["overall_prediction"]-prediction["label"]).mean()
      std1=prediction["Sq_error"].sum()
      div=std1/(test_df.shape[0]-1)
      std_dev=math.sqrt(div)
      print("Mean error is: ")
      print(meanerror)
      print("Standard Deviation is: ")
      print(std_dev)
      return mse

      
def main():
	input_path=input("Enter the input path: ")
	dataset,regdata=take_input(input_path)
	print("Dataset read complete----------------")
	print("Processing dataset-------------------")
	df,df1=processing_data(dataset,regdata)
	print("Processing data complete-------------")
	print("What do you want")
	print("1. Decision Tree\n2. Bagged Decision Tree\n3. Random Forest")
	wish=input()
	print("Classification or Regression ?")
	wish2=input()
	if(wish2=="Regression"):
		regtrain_df,regtest_df=split_data_haha(df1)
	if(wish2=="Classification"):
		train_df,test_df=split_data_haha(df)	
	max_depth=int(input("What max_depth of tree do you want: "))
	if(wish=="Decision Tree"):
		if(wish2=="Classification"):
			metric=calculate_metric(train_df=train_df,task="Classification",test_df=test_df,max_depth=max_depth)
			print("Accuracy of Decision Tree is :")
			print(metric)
		if(wish2=="Regression"):
			metric=calculate_metric(train_df=regtrain_df,task="Regression",test_df=regtest_df,max_depth=max_depth)
			print("Mean Square error of Decision Tree is: ")
			print(metric)
	if (wish=="Bagged Decision Tree"):
		n_trees=int(input("How many bagged trees: "))
		n_bootstrap=int(input("How many samples in one bag: "))
		if(wish2=="Classification"):
			metric=bagging_predictions(df=train_df,n_trees=n_trees,n_bootstrap=n_bootstrap,max_depth=max_depth,want_random_forest=False,task="Classification",test_df=test_df)
			print("Accuracy of Bagged Decision Tree is: ")
			print(metric)
		if(wish2=="Regression"):
			metric=bagging_predictions(df=regtrain_df,n_trees=n_trees,n_bootstrap=n_bootstrap,max_depth=max_depth,want_random_forest=False,task="Regression",test_df=regtest_df)
			print("Mean Square error of bagged de Tree is: ")
			print(metric)
	if (wish=="Random Forest"):
		n_trees=int(input("How many bagged trees: "))
		n_bootstrap=int(input("How many samples in one bag: "))
		n_features=int(input("How many random features: "))
		if(wish2=="Classification"):
			metric=bagging_predictions(df=train_df,n_trees=n_trees,n_bootstrap=n_bootstrap,max_depth=max_depth,want_random_forest=True,random_feat=n_features,task="Classification",test_df=test_df)
			print("Accuracy of Random Forest is: ")
			print(metric)
		if(wish2=="Regression"):
			metric=bagging_predictions(df=regtrain_df,n_trees=n_trees,n_bootstrap=n_bootstrap,max_depth=max_depth,want_random_forest=True,random_feat=n_features,task="Regression",test_df=regtest_df)
			print("Mean Square error of Random Forest is: ")
			print(metric)
	print("---------------END----------------------------------")		




if __name__ == '__main__':
	main()